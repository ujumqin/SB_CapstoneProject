bing_data <- data.frame(bing_data$text_topic, bing_data$sentimentscore, bing_data$negative)
names(bing_data) <- c("forum_text","sentiment_score","negative")
return(bing_data)
}
forum_bing <- bing_maker(tidy_forums)
reddit_bing <- bing_maker(tidy_reddit)
forandred_bing <- bing_maker(tidy_forandred)
afinn_maker <- function(tidy_data) {
afinn_data <- tidy_data %>%
inner_join(get_sentiments("afinn"))
#create this column to help calculate sentiment score
afinn_data$count <- 1
#calculate sentiment score using afinn
afinn_data <- afinn_data %>%
group_by(text_topic) %>%
mutate(sentimentscore = sum(score)/sum(count))
#collapse the duplicated columns by text_topic
afinn_data <- afinn_data[!duplicated(afinn_data$text_topic),]
afinn_data <- data.frame(afinn_data$text_topic, afinn_data$sentimentscore)
names(afinn_data) <- c("forum_text","sentiment_score")
afinn_data$negative <- 0
#for loops codes the sentiment from bing sentiment into a -1 or 1. 1 is positive and -1 is negative. Also counts the number of sentiment words to be used later.
for(i in 1:length(afinn_data$sentiment_score)) {
if(afinn_data$sentiment_score[i] < 0) {
afinn_data$negative[i] = "TRUE"
} else {
afinn_data$negative[i] = "FALSE"
}
}
return(afinn_data)
}
forum_afinn <- afinn_maker(tidy_forums)
forum_afinn$negative <- as.factor(forum_afinn$negative)
reddit_afinn <- afinn_maker(tidy_reddit)
reddit_afinn$negative <- as.factor(reddit_afinn$negative)
forandred_afinn <- afinn_maker(tidy_forandred)
forandred_afinn$negative <- as.factor(forandred_afinn$negative)
corpus_maker <- function(sentiment_data) {
data_corpus <- Corpus(VectorSource(sentiment_data$forum_text))
data_corpus <- tm_map(data_corpus, removePunctuation)
data_corpus <- tm_map(data_corpus, tolower)
data_corpus <- tm_map(data_corpus, removeWords, stopwords("english"))
data_corpus <- tm_map(data_corpus, stemDocument)
return(data_corpus)
}
bforum_corpus <- corpus_maker(forum_bing)
breddit_corpus <- corpus_maker(reddit_bing)
bforandred_corpus <- corpus_maker(forandred_bing)
aforum_corpus <- corpus_maker(forum_afinn)
areddit_corpus <- corpus_maker(reddit_afinn)
aforandred_corpus <- corpus_maker(forandred_afinn)
process_corpus <- function(corpus_name, bing_name) {
#Find frequencies of words
freq <- DocumentTermMatrix(corpus_name)
#removing the sparse terms
sparse <- removeSparseTerms(freq, 0.995)
#convert sparse data into a data frame
sparse <- as.data.frame(as.matrix(sparse))
#converts variable names to appropriate names (some may start with numbers)
colnames(sparse) = make.names(colnames(sparse))
#add dependent variable to the dataframe. We will be predicting this value.
sparse$negative <- bing_name$negative
return(sparse)
}
b_forum_predict <- process_corpus(bforum_corpus, forum_bing)
b_reddit_predict <- process_corpus(breddit_corpus, reddit_bing)
b_forandred_predict <- process_corpus(bforandred_corpus, forandred_bing)
a_forum_predict <- process_corpus(aforum_corpus, forum_afinn)
a_reddit_predict <- process_corpus(areddit_corpus, reddit_afinn)
a_forandred_predict <- process_corpus(aforandred_corpus, forandred_afinn)
set.seed(1113)
sr <- 0.2
bforum_split <- sample.split(b_forum_predict$negative, SplitRatio = sr)
bforum_test <- subset(b_forum_predict, bforum_split == TRUE)
bforum_train <- subset(b_forum_predict, bforum_split == FALSE)
breddit_split <- sample.split(b_reddit_predict$negative, SplitRatio = sr)
breddit_test <- subset(b_reddit_predict, breddit_split == TRUE)
breddit_train <- subset(b_reddit_predict, breddit_split == FALSE)
bforandred_split <- sample.split(b_forandred_predict$negative, SplitRatio = sr)
bforandred_test <- subset(b_forandred_predict, bforandred_split == TRUE)
bforandred_train <- subset(b_forandred_predict, bforandred_split == FALSE)
aforum_split <- sample.split(a_forum_predict$negative, SplitRatio = sr)
aforum_test <- subset(a_forum_predict, aforum_split == TRUE)
aforum_train <- subset(a_forum_predict, aforum_split == FALSE)
areddit_split <- sample.split(a_reddit_predict$negative, SplitRatio = sr)
areddit_test <- subset(a_reddit_predict, areddit_split == TRUE)
areddit_train <- subset(a_reddit_predict, areddit_split == FALSE)
aforandred_split <- sample.split(a_forandred_predict$negative, SplitRatio = sr)
aforandred_test <- subset(a_forandred_predict, aforandred_split == TRUE)
aforandred_train <- subset(a_forandred_predict, aforandred_split == FALSE)
CARTMODELMAKER <- function(traindata) {
cartmodel1 <- rpart(negative~., data = traindata, method = "class")
return(cartmodel1)
}
fitOWControl = trainControl(method="cv",number=10)
cartGrid <- expand.grid(.cp=(1:50)*0.00001)
cpmaker <- function(traindata,fold,increment) {
fitOWControl = trainControl(method="cv",number=fold)
cartGrid <- expand.grid(.cp=(1:50)*increment)
cpvalue <- train(negative ~., data=traindata, method="rpart", trControl=fitOWControl, tuneGrid=cartGrid)
return(cpvalue)
}
CARTMODEL2 <- function(traindata, cp) {
cartmodel2train <-  rpart(negative ~., data=traindata, method="class",control=rpart.control(cp=cp))
}
RFMODEL <- function(trainingset) {
randomforestmodel <- randomForest(negative ~., data = trainingset)
}
CARTMODELTEST <- function(model,testdata) {
predictmodel <- predict(model, newdata=testdata, type="class")
return(predictmodel)
}
tablechecker <- function(testdata, modelresults) {
tablecheck <- table(testdata$negative, modelresults)
confusionMatrix(tablecheck)
}
bingforummodel1 <- CARTMODELMAKER(bforum_train)
bingredditmodel1 <- CARTMODELMAKER(breddit_train)
bingforandredmode1l <- CARTMODELMAKER(bforandred_train)
prp(bingforummodel1)
prp(bingredditmodel1)
prp(bingforummodel1)
bingforummodel2 <- CARTMODEL2(bforum_train, 0.002)
bingredditmodel2 <- CARTMODEL2(breddit_train, 0.002)
bingforandredmodel2 <- CARTMODEL2(bforandred_train, 0.003)
prp(bingforummodel2)
prp(bingredditmodel2)
prp(bingforandredmodel2)
bingforummodel3 <- RFMODEL(bforum_train)
bingredditmodel3 <- RFMODEL(breddit_train)
bingforandredmodel3 <- RFMODEL(bforandred_train)
afinnforandredmodel3 <- RFMODEL(aforandred_train)
tablechecker(aforandred_test, afinnforandredtest3)
afinnforandredtest3 <- CARTMODELTEST(afinnforandredmodel3, aforandred_test)
tablechecker(aforandred_test, afinnforandredtest3)
library(rvest)
library(stringr)
library(tidyr)
library(RCurl)
library(rvest)
library(stringr)
library(tidyr)
library(RCurl)
library(dplyr)
library(curl)
ow.forums <- read_html("https://us.battle.net/forums/en/overwatch/22813879/")
links <- ow.forums %>% html_nodes("a") %>% html_attr("href")
links <- as.data.frame(links, row.names = NULL)
keep<- grepl("^/forums", links$links)
keep <- as.data.frame(keep, row.names = NULL)
links <- cbind(links, keep)
links <- links[links$keep == "TRUE",]
front_url <- "https://us.battle.net"
full_links <- paste(front_url,links$links, sep = "")
clean_links <- unique(full_links)
ow.total <- data.frame("link", "title", "time", "text", stringsAsFactors = FALSE)
n <- 1
while(n <= length(clean_links)) {
read_single_links <- read_html(clean_links[n])
title_results <- read_single_links %>% html_nodes(".Topic-title")
title <- xml_contents(title_results) %>% html_text(trim = TRUE)
#line for testing to see new threads easier
print("---------------------------------------------------------------------------------------------------------")
print(clean_links[n])
print(title)
#pulls content of thread
topic_results <- read_single_links %>% html_nodes(".TopicPost-bodyContent")
topic <- trimws(topic_results)
#cleanup of bodycontent text.
clean_topic <- gsub("<.*?>", "", topic)
clean_topic <- gsub("\n", "", clean_topic)
#pulls date
topic_time <- read_single_links %>% html_node(".TopicPost-timestamp")
clean_time <- gsub('<.*content=\\"', "", topic_time)
clean_time <- gsub('\">.*', "", clean_time)
clean_time <- as.POSIXct(clean_time, format = "%m/%d/%Y %I:%M %p")
clean_time <- paste(clean_time)
print(clean_time)
#adds topic to a vector with link, title using a for loop
for(i in 1:length(clean_topic)) {
ow.total <- rbind(ow.total, c(clean_links[n], title, clean_time[i], clean_topic[i]))
}
print(n)
n<-n+1
}
newdate <- c()
newtext <- c()
for(i in 1:length(ow.total$X.text.)) {
if(grepl('\\d\\d/\\d\\d/.*?M', ow.total$X.text.[i]) == FALSE) {
print("CLEAN")
} else {
print("UNCLEAN! CLEANING")
m <- regexpr('\\d\\d/\\d\\d/.*?M',ow.total$X.text.[i])
owfdates <- regmatches(ow.total$X.text.[i], m, invert = FALSE)
owcleantext <- gsub('\\d\\d/\\d\\d/.*?M', '', ow.total$X.text.[i])
newdate[i] <- owfdates
newtext[i] <- owcleantext
}
}
for(i in 1:length(ow.total$X.text.)) {
if(is.na(newtext[i])) {
print("Do not write over old value")
} else if(newtext[i] != 'NA') {
#print("Copy over new data")
ow.total$X.text.[i] <- newtext[i]
ow.total$X.time.[i] <- newdate[i]
}
}
str(ow.total)
newow.total_clean <- ow.total[!duplicated(ow.total[4]),]
write.csv(newow.total_clean, 'OWFORUMS_manualcheck_12_30.csv')
library(rvest)
library(tidyr)
library(RCurl)
library(dplyr)
library(curl)
ow.forums <- read_html("https://www.reddit.com/r/Overwatch/")
links <- ow.forums %>% html_nodes("a") %>% html_attr("href")
links <- as.data.frame(links, row.names = NULL)
keep<- grepl("^https://www.reddit.com/r/Overwatch/comments/", links$links)
keep <- as.data.frame(keep, row.names = NULL)
links <- cbind(links, keep)
links <- links[links$keep == "TRUE",]
links <- paste(links$links)
owreddit.total <- data.frame("link", "title", "time", "text", "user", stringsAsFactors = FALSE)
print(owreddit.total)
n <- 1
while(n <= length(links)) {
read_single_links <- read_html(links[n])
#scrapes the link's title
link_title <- gsub("https://www.reddit.com/r/Overwatch/comments/", "", links)
link_title <- substr(link_title,8, nchar(link_title)-1)
link_title <- gsub("_", " ", link_title)
#scrapes comments
comments <- read_single_links %>% html_nodes(".md")
#convert from xml to usable format
comments <- trimws(comments)
comments <- gsub("<.*?>", "", comments)
#script going too fast and wasn't cleaning
comments <- gsub("\n", "", comments)
#scrapes the .tagline node which has username data
reddit_user <- read_single_links %>% html_nodes(".tagline")
#Scrapes to the user name
reddit_user <- gsub('.*user/', "", reddit_user)
#Scrapes the stuff after class out
reddit_user <- gsub('class.*', "", reddit_user)
#clean up
reddit_user <- gsub(" ","",reddit_user, fixed=TRUE)
reddit_user <- gsub("\"","",reddit_user, fixed=TRUE)
reddit_user <- gsub("<p","",reddit_user, fixed=TRUE)
#scrapes comment timestamp by date
comment_time <- read_single_links %>% html_nodes(".tagline, time")
comment_time <- gsub('.*datetime=', "", comment_time)
comment_time <- gsub('T.*', "", comment_time)
comment_time <- gsub('\"', "", comment_time)
comment_time <- gsub('<p.*', "", comment_time)
#prints everything scrapped
print("---------------------------------------------------------------------------------------------------------")
print("====link====")
print(links[n])
print(" ")
print("====title====")
print(link_title[n])
print(" ")
print("====comment====")
print(comments[n])
print(" ")
print("====user====")
print(reddit_user[n])
print(" ")
print("====date====")
print(comment_time[n])
print(" ")
#adds topic to a vector with link, title using a for loop
for(i in 1:length(comments)) {
owreddit.total <- rbind(owreddit.total, c(links[n], link_title[n], comment_time[i], comments[i+2], reddit_user[i+1]))
}
print(n)
n<-n+1
#Sys.sleep(2)
}
reddit <- read.csv(text=getURL("https://raw.githubusercontent.com/ujumqin/SB_CapstoneProject/master/Final%20Project%20Files/OW%20Data/Raw%20Files/REDDIT_12_22_FINAL.csv"), stringsAsFactors = FALSE)
twitter <- read.csv(text=getURL("https://raw.githubusercontent.com/ujumqin/SB_CapstoneProject/master/Final%20Project%20Files/OW%20Data/Raw%20Files/OWTWITTER_12-22_FINAL.csv"), stringsAsFactors = FALSE)
forums <- read.csv(text=getURL("https://raw.githubusercontent.com/ujumqin/SB_CapstoneProject/master/Final%20Project%20Files/OW%20Data/Raw%20Files/OWFORUMS12_22_FINAL.csv"), stringsAsFactors = FALSE)
withouttwitter <- read.csv(text=getURL("https://raw.githubusercontent.com/ujumqin/SB_CapstoneProject/master/Final%20Project%20Files/OW%20Data/Raw%20Files/OWFORUMS%26REDDIT_12_22_FINAL.csv"), stringsAsFactors = FALSE)
reddit$text_topic <- reddit$X.text.
twitter$text_topic <- twitter$text
forums$text_topic <- forums$text
all$text_topic <- all$text
withouttwitter$text_topic <- withouttwitter$text
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(RCurl)
reddit$text_topic <- reddit$X.text.
twitter$text_topic <- twitter$text
forums$text_topic <- forums$text
all$text_topic <- all$text
withouttwitter$text_topic <- withouttwitter$text
tidy_reddit <- reddit %>%
group_by(text_topic) %>%
dplyr::mutate(linenumber = row_number()) %>%
unnest_tokens(word, X.text.) %>%
ungroup()
tidy_twitter <- twitter %>%
group_by(text_topic) %>%
dplyr::mutate(linenumber = row_number()) %>%
unnest_tokens(word, text) %>%
ungroup()
tidy_forums <- forums %>%
group_by(X.) %>%
dplyr::mutate(linenumber = row_number()) %>%
unnest_tokens(word, text) %>%
ungroup()
tidy_notwitter <- withouttwitter %>%
group_by(X.) %>%
dplyr::mutate(linenumber = row_number()) %>%
unnest_tokens(word, text) %>%
ungroup()
reddit_bing <- tidy_reddit %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
twitter_bing <- tidy_twitter %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
forum_bing <- tidy_forums %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
all_bing <- tidy_all %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
withouttwitter_bing <- tidy_notwitter %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
reddit_bing %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Reddit Most Used Sentiment Words",
x = NULL) +
coord_flip()
twitter_bing %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Twitter Most Used Sentiment Words",
x = NULL) +
coord_flip()
forum_bing %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Overwatch Forums Most Used Sentiment Words",
x = NULL) +
coord_flip()
all_bing %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Most Used Sentiment Words By Overwatch Community",
x = NULL) +
coord_flip()
withouttwitter_bing %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Most Used Sentiment Words By Overwatch Community",
x = NULL) +
coord_flip()
reddit_nrc <- tidy_reddit %>%
inner_join(get_sentiments("nrc"))
twitter_nrc <- tidy_twitter %>%
inner_join(get_sentiments("nrc"))
forum_nrc <- tidy_forums %>%
inner_join(get_sentiments("nrc"))
all_nrc <- tidy_all %>%
inner_join(get_sentiments("nrc"))
withouttwitter_nrc <- tidy_all %>%
inner_join(get_sentiments("nrc"))
ggplot(reddit_nrc, aes(x="", fill=sentiment)) +
geom_bar(width = 1) +
ggtitle("reddit")
reddit_bar
reddit_bar + coord_polar("y")
ggplot(forum_nrc, aes(x="", fill=sentiment)) +
geom_bar(width = 1) +
ggtitle("Overwatch Forums")
forum_bar
forum_bar + coord_polar("y")
ggplot(twitter_nrc, aes(x=sentiment)) +
geom_bar(aes(y=..count.., fill =..count..)) +
theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Twitter") +
coord_cartesian(ylim = c(0,11000), expand = TRUE)
ggplot(forum_nrc, aes(x=sentiment)) +
geom_bar(aes(y=..count.., fill =..count..)) +
theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Overwatch Forums") +
coord_cartesian(ylim = c(0,11000), expand = TRUE)
ggplot(withouttwitter_nrc, aes(x=sentiment)) +
geom_bar(aes(y=..count.., fill =..count..)) +
theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1, vjust = 1)) +
ggtitle("All Communities") +
coord_cartesian(ylim = c(0,24000), expand = TRUE)
reddit_afinn <- tidy_reddit %>%
inner_join(get_sentiments("afinn"))
twitter_afinn <- tidy_twitter %>%
inner_join(get_sentiments("afinn"))
forum_afinn <- tidy_forums %>%
inner_join(get_sentiments("afinn"))
withouttwitter_afinn <- tidy_notwitter %>%
inner_join(get_sentiments("afinn"))
ggplot(twitter_afinn, aes(x=score)) +
geom_bar(aes(y=..count.., fill= ..count..)) +
labs(x="afinn score") +
xlab("afinn score")+
coord_cartesian(ylim = c(0,7000), expand = TRUE) +
ggtitle("twitter") +
scale_x_continuous(breaks = -5:5)
ggplot(forum_afinn, aes(x=score)) +
geom_bar(aes(y=..count.., fill=..count..)) +
labs(x="afinn score") +
xlab("afinn score")+
coord_cartesian(ylim = c(0,7000), expand = TRUE)+
ggtitle("Overwatch Forums") +
scale_x_continuous(breaks = -5:5)
ggplot(reddit_afinn, aes(x=score)) +
geom_bar(aes(y=..count.., fill=..count..)) +
labs(x="afinn score") +
xlab("afinn score")  +
coord_cartesian(ylim = c(0,7000), expand = TRUE)+
ggtitle("Reddit") +
scale_x_continuous(breaks = -5:5)
ggplot(withouttwitter_afinn, aes(x=score)) +
geom_bar(aes(y=..count.., fill=..count..)) +
labs(x="afinn score") +
xlab("afinn score")  +
coord_cartesian(ylim = c(0,13000), expand = TRUE)+
ggtitle("All") +
scale_x_continuous(breaks = -5:5)
tidy_forums %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
tidy_reddit %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
tidy_twitter %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
tidy_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
tidy_notwitter %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
par(mfrow=c(2,2))
tidy_forums %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 80)
tidy_reddit %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 80)
tidy_twitter %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 80)
tidy_notwitter %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 80)
